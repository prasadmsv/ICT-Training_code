{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddb4261",
   "metadata": {},
   "source": [
    "## Understanding Generation Configuration\n",
    "Display and explain default generation parameters and their meanings.\n",
    "- **Temperature**: Controls randomness (0.0 = deterministic, 1.0 = highly creative)\n",
    "- **Top-p**: Nucleus sampling threshold (focuses on most probable tokens)\n",
    "- **Top-k**: Limits vocabulary to top K most likely tokens\n",
    "- **Max tokens**: Maximum length of generated response\n",
    "- **Stop sequences**: Specific text patterns that end generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_default_config():\n",
    "    \"\"\"Display default ChatGPT generation configuration\"\"\"\n",
    "    print(\"üìã Default ChatGPT Generation Configuration\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # ChatGPT does NOT expose a default config object like Gemini.\n",
    "    # So we define the commonly used defaults explicitly:\n",
    "    config_dict = {\n",
    "        \"temperature\": 1.0,        # typical default\n",
    "        \"top_p\": 1.0,              # default nucleus sampling\n",
    "        \"max_output_tokens\": \"Model-dependent (can vary by model)\",\n",
    "        \"presence_penalty\": 0.0,\n",
    "        \"frequency_penalty\": 0.0,\n",
    "        \"stop\": None               # no stop sequence by default\n",
    "    }\n",
    "\n",
    "    # Print config values\n",
    "    for param, value in config_dict.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "\n",
    "    # Explanations\n",
    "    print(\"\\nüìö Parameter Explanations:\")\n",
    "    print(\"   ‚Ä¢ Temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)\")\n",
    "    print(\"   ‚Ä¢ Top-p: Nucleus sampling threshold (0.1 = focused, 1.0 = fully open)\")\n",
    "    print(\"   ‚Ä¢ Max tokens: Maximum tokens ChatGPT is allowed to generate\")\n",
    "    print(\"   ‚Ä¢ Presence penalty: Penalizes repeating topics\")\n",
    "    print(\"   ‚Ä¢ Frequency penalty: Penalizes repeating exact words\")\n",
    "    print(\"   ‚Ä¢ Stop: Sequence(s) that cause generation to stop\")\n",
    "\n",
    "# Run it\n",
    "display_default_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93567288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "%pip install python-dotenv openai\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = os.getenv(\"OPENAI_MODEL\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not found. Create a .env file with OPENAI_API_KEY=<your_key> \"\n",
    "        \"or set the environment variable in your system.\"\n",
    "    )\n",
    "\n",
    "print(\"Using model:\", MODEL)\n",
    "print(\"OpenAI API Key set:\", API_KEY is not None)\n",
    "\n",
    "# Single shared client for the notebook (do NOT hardcode keys elsewhere)\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed2489",
   "metadata": {},
   "source": [
    "## Temperature Parameter Deep Dive\n",
    "Compare different temperature values (0.0, 0.3, 0.7, 1.0) using the same creative writing prompt to show how randomness affects output creativity and consistency.\n",
    "\n",
    "### Temperature Parameter Deep Dive\n",
    "\n",
    "How temperature affects creativity, unpredictability, and the overall style of an AI‚Äôs response\n",
    "\n",
    "The temperature setting is one of the most important concepts in prompt engineering, because it directly controls how the model behaves.\n",
    "Even when you use the exact same prompt, changing the temperature can dramatically change the output‚Äôs tone, originality, and consistency.\n",
    "\n",
    "Think of temperature as a ‚Äúcreativity knob.‚Äù\n",
    "Turn it down, the model becomes serious and predictable.\n",
    "Turn it up, the model becomes imaginative and expressive.\n",
    "\n",
    "We will compare four temperature values: 0.0, 0.3, 0.7, and 1.0, using the same writing prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04854e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)\n",
    "#client.models.retrieve(MODEL)  # Verify model exists\n",
    "\n",
    "def temperature_comparison():\n",
    "    \"\"\"Compare different temperature settings with the same prompt using ChatGPT\"\"\"\n",
    "    print(\"üå°Ô∏è  Temperature Parameter Comparison\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    base_prompt = (\n",
    "        \"Write about FUJIFILM Apeos Printers \"\n",
    "        \"Make it 2 paragraphs.\"\n",
    "    )\n",
    "\n",
    "    temperatures = [0.0, 0.3, 0.7, 1.0]\n",
    "\n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nüå°Ô∏è  TEMPERATURE: {temp}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        try:\n",
    "            # ChatGPT Responses API with temperature control\n",
    "            response = client.responses.create(                              \n",
    "                input=base_prompt,\n",
    "                temperature=temp,\n",
    "                max_output_tokens=30,\n",
    "                model=MODEL\n",
    "            )\n",
    "\n",
    "            output_text = response.output_text\n",
    "\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output_text)\n",
    "\n",
    "            print(f\"\\nüìä Response Length: {len(output_text)} characters\")\n",
    "            print(f\"üìä Word Count: {len(output_text.split())} words\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Run the test\n",
    "temperature_comparison()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def temperature_comparison_html():\n",
    "    \"\"\"Compare different temperature settings using ChatGPT and output as HTML table\"\"\"\n",
    "    \n",
    "    print(\"üå°Ô∏è  Temperature Parameter Comparison\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Base prompt about Fujifilm Apeos Printers\n",
    "    base_prompt = (\n",
    "        \"Write about FUJIFILM Apeos Printers. \"\n",
    "        \"Make it 2 paragraphs.\"\n",
    "    )\n",
    "    \n",
    "    temperatures = [0.0, 0.3, 0.7, 1.0]\n",
    "    \n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Start building HTML table\n",
    "    html_output = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Temperature Comparison - ChatGPT</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            max-width: 1600px;\n",
    "            margin: 40px auto;\n",
    "            padding: 20px;\n",
    "            background: #f5f5f5;\n",
    "        }\n",
    "        .header {\n",
    "            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);\n",
    "            color: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 10px;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .header h1 {\n",
    "            margin: 0;\n",
    "            font-size: 32px;\n",
    "        }\n",
    "        .fujifilm-logo {\n",
    "            color: #e60012;\n",
    "            font-weight: bold;\n",
    "            font-size: 24px;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        .prompt-box {\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 30px;\n",
    "            border-left: 4px solid #e60012;\n",
    "        }\n",
    "        .temp-scale {\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 30px;\n",
    "            text-align: center;\n",
    "        }\n",
    "        .gradient-bar {\n",
    "            height: 40px;\n",
    "            background: linear-gradient(to right, #2196f3, #4caf50, #ff9800, #f44336);\n",
    "            border-radius: 20px;\n",
    "            margin: 20px 0;\n",
    "            position: relative;\n",
    "        }\n",
    "        .gradient-label {\n",
    "            display: flex;\n",
    "            justify-content: space-between;\n",
    "            font-size: 14px;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        table {\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            background: white;\n",
    "            box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        thead {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "        }\n",
    "        th {\n",
    "            padding: 15px;\n",
    "            text-align: left;\n",
    "            font-weight: 600;\n",
    "        }\n",
    "        td {\n",
    "            padding: 15px;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "            vertical-align: top;\n",
    "        }\n",
    "        tr:hover {\n",
    "            background: rgba(0,0,0,0.02);\n",
    "        }\n",
    "        .temp-0 { background: #e3f2fd; }\n",
    "        .temp-03 { background: #e8f5e9; }\n",
    "        .temp-07 { background: #fff3e0; }\n",
    "        .temp-10 { background: #ffebee; }\n",
    "        .temperature-cell {\n",
    "            font-weight: bold;\n",
    "            font-size: 24px;\n",
    "        }\n",
    "        .temp-0-color { color: #2196f3; }\n",
    "        .temp-03-color { color: #4caf50; }\n",
    "        .temp-07-color { color: #ff9800; }\n",
    "        .temp-10-color { color: #f44336; }\n",
    "        .output-text {\n",
    "            line-height: 1.8;\n",
    "            font-size: 14px;\n",
    "        }\n",
    "        .stats {\n",
    "            text-align: center;\n",
    "            font-weight: 600;\n",
    "            font-size: 16px;\n",
    "        }\n",
    "        .characteristics {\n",
    "            font-size: 13px;\n",
    "            color: #666;\n",
    "            font-style: italic;\n",
    "        }\n",
    "        .info-box {\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            border-left: 4px solid #4caf50;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .comparison-grid {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(2, 1fr);\n",
    "            gap: 20px;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .comparison-card {\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        .comparison-card h4 {\n",
    "            margin-top: 0;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <div class=\"fujifilm-logo\">FUJIFILM</div>\n",
    "        <h1>üå°Ô∏è  Temperature Parameter Comparison - ChatGPT (GPT-4)</h1>\n",
    "        <p style=\"margin: 10px 0 0 0; font-size: 18px;\">Understanding Creativity vs. Determinism</p>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"prompt-box\">\n",
    "        <strong>üìã Base Prompt:</strong> \"\"\" + base_prompt + \"\"\"\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"temp-scale\">\n",
    "        <h3>Temperature Scale: 0.0 (Deterministic) ‚Üí 1.0 (Creative)</h3>\n",
    "        <div class=\"gradient-bar\"></div>\n",
    "        <div class=\"gradient-label\">\n",
    "            <span>‚ùÑÔ∏è Factual & Consistent</span>\n",
    "            <span>üî• Creative & Varied</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th style=\"width: 8%;\">Temp</th>\n",
    "                <th style=\"width: 12%;\">Category</th>\n",
    "                <th style=\"width: 50%;\">Generated Response</th>\n",
    "                <th style=\"width: 10%; text-align: center;\">Characters</th>\n",
    "                <th style=\"width: 10%; text-align: center;\">Words</th>\n",
    "                <th style=\"width: 10%; text-align: center;\">Unique Words</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Generate responses for each temperature\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nüå°Ô∏è  TEMPERATURE: {temp}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # ChatGPT API call with temperature control\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": base_prompt}\n",
    "                ],\n",
    "                temperature=temp,\n",
    "                max_tokens=200  # Enough for 2 paragraphs\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content\n",
    "            \n",
    "            # Calculate statistics\n",
    "            char_length = len(output_text)\n",
    "            word_count = len(output_text.split())\n",
    "            unique_words = len(set(output_text.lower().split()))\n",
    "            \n",
    "            # Determine characteristics based on temperature\n",
    "            if temp == 0.0:\n",
    "                category = \"Deterministic\"\n",
    "                characteristics = \"Most predictable, factual, consistent across runs\"\n",
    "                row_class = \"temp-0\"\n",
    "                color_class = \"temp-0-color\"\n",
    "            elif temp <= 0.4:\n",
    "                category = \"Low Creativity\"\n",
    "                characteristics = \"Reliable, focused, minimal variation\"\n",
    "                row_class = \"temp-03\"\n",
    "                color_class = \"temp-03-color\"\n",
    "            elif temp <= 0.8:\n",
    "                category = \"Balanced\"\n",
    "                characteristics = \"Good mix of creativity and coherence\"\n",
    "                row_class = \"temp-07\"\n",
    "                color_class = \"temp-07-color\"\n",
    "            else:\n",
    "                category = \"High Creativity\"\n",
    "                characteristics = \"Most creative, varied, experimental\"\n",
    "                row_class = \"temp-10\"\n",
    "                color_class = \"temp-10-color\"\n",
    "            \n",
    "            # Print to console\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output_text)\n",
    "            print(f\"\\nüìä Response Length: {char_length} characters\")\n",
    "            print(f\"üìä Word Count: {word_count} words\")\n",
    "            print(f\"üìä Unique Words: {unique_words}\")\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'temperature': temp,\n",
    "                'category': category,\n",
    "                'output': output_text,\n",
    "                'char_length': char_length,\n",
    "                'word_count': word_count,\n",
    "                'unique_words': unique_words,\n",
    "                'characteristics': characteristics\n",
    "            })\n",
    "            \n",
    "            # Add row to HTML table\n",
    "            html_output += f\"\"\"\n",
    "            <tr class=\"{row_class}\">\n",
    "                <td class=\"temperature-cell {color_class}\">{temp}</td>\n",
    "                <td><strong>{category}</strong><br><span class=\"characteristics\">{characteristics}</span></td>\n",
    "                <td class=\"output-text\">{output_text}</td>\n",
    "                <td class=\"stats\">{char_length}</td>\n",
    "                <td class=\"stats\">{word_count}</td>\n",
    "                <td class=\"stats\">{unique_words}</td>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            html_output += f\"\"\"\n",
    "            <tr>\n",
    "                <td class=\"temperature-cell\">{temp}</td>\n",
    "                <td colspan=\"5\" style=\"color: red;\">Error: {str(e)}</td>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    # Close table\n",
    "    html_output += \"\"\"\n",
    "        </tbody>\n",
    "    </table>\n",
    "\"\"\"\n",
    "    \n",
    "    # Add side-by-side comparison if we have results\n",
    "    if len(results) >= 2:\n",
    "        html_output += \"\"\"\n",
    "    <div class=\"comparison-grid\">\n",
    "        <div class=\"comparison-card\">\n",
    "            <h4>üîµ Temperature 0.0 - Most Deterministic</h4>\n",
    "            <p><strong>Use for:</strong></p>\n",
    "            <ul>\n",
    "                <li>Technical specifications</li>\n",
    "                <li>Product datasheets</li>\n",
    "                <li>Fact-based documentation</li>\n",
    "                <li>Consistent formatting</li>\n",
    "            </ul>\n",
    "            <p><strong>Example:</strong> \"List the exact specifications of the Apeos C325 printer\"</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"comparison-card\">\n",
    "            <h4>üü¢ Temperature 0.3 - Low Creativity</h4>\n",
    "            <p><strong>Use for:</strong></p>\n",
    "            <ul>\n",
    "                <li>Customer support responses</li>\n",
    "                <li>Email templates</li>\n",
    "                <li>Product descriptions</li>\n",
    "                <li>Troubleshooting guides</li>\n",
    "            </ul>\n",
    "            <p><strong>Example:</strong> \"Explain how to set up wireless printing on Apeos printers\"</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"comparison-card\">\n",
    "            <h4>üü† Temperature 0.7 - Balanced</h4>\n",
    "            <p><strong>Use for:</strong></p>\n",
    "            <ul>\n",
    "                <li>Blog posts</li>\n",
    "                <li>Marketing content</li>\n",
    "                <li>Social media posts</li>\n",
    "                <li>General business writing</li>\n",
    "            </ul>\n",
    "            <p><strong>Example:</strong> \"Write an engaging blog post about sustainable printing\"</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"comparison-card\">\n",
    "            <h4>üî¥ Temperature 1.0 - High Creativity</h4>\n",
    "            <p><strong>Use for:</strong></p>\n",
    "            <ul>\n",
    "                <li>Creative taglines</li>\n",
    "                <li>Brainstorming ideas</li>\n",
    "                <li>Unique campaign concepts</li>\n",
    "                <li>Experimental content</li>\n",
    "            </ul>\n",
    "            <p><strong>Example:</strong> \"Create 5 unexpected taglines for Instax cameras\"</p>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "    \n",
    "    # Add insights section\n",
    "    html_output += \"\"\"\n",
    "    <div class=\"info-box\">\n",
    "        <h3>üí° Key Insights About Temperature Parameter</h3>\n",
    "        <ul>\n",
    "            <li><strong>What is Temperature?</strong> Controls randomness in token selection. Lower = more predictable, Higher = more creative.</li>\n",
    "            <li><strong>Temperature 0.0:</strong> Always picks the most likely next token. Run it 10 times, get nearly identical results.</li>\n",
    "            <li><strong>Temperature 0.3-0.5:</strong> Slight variation while maintaining consistency. Good for business content.</li>\n",
    "            <li><strong>Temperature 0.7-0.8:</strong> Balanced creativity. Most versatile for general use.</li>\n",
    "            <li><strong>Temperature 1.0+:</strong> High variability. Can be unpredictable but generates unique content.</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3 style=\"margin-top: 25px;\">üéØ Temperature Guidelines for Fujifilm Content</h3>\n",
    "        <table style=\"width: 100%; margin-top: 15px;\">\n",
    "            <tr style=\"background: #f5f5f5;\">\n",
    "                <th style=\"padding: 10px;\">Content Type</th>\n",
    "                <th style=\"padding: 10px;\">Recommended Temperature</th>\n",
    "                <th style=\"padding: 10px;\">Reason</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 10px;\">Product Specifications</td>\n",
    "                <td style=\"padding: 10px; text-align: center;\"><strong>0.0 - 0.2</strong></td>\n",
    "                <td style=\"padding: 10px;\">Must be factual and consistent</td>\n",
    "            </tr>\n",
    "            <tr style=\"background: #f5f5f5;\">\n",
    "                <td style=\"padding: 10px;\">Technical Documentation</td>\n",
    "                <td style=\"padding: 10px; text-align: center;\"><strong>0.2 - 0.3</strong></td>\n",
    "                <td style=\"padding: 10px;\">Needs clarity with slight variation</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 10px;\">Product Descriptions</td>\n",
    "                <td style=\"padding: 10px; text-align: center;\"><strong>0.5 - 0.7</strong></td>\n",
    "                <td style=\"padding: 10px;\">Balance between facts and appeal</td>\n",
    "            </tr>\n",
    "            <tr style=\"background: #f5f5f5;\">\n",
    "                <td style=\"padding: 10px;\">Social Media Posts</td>\n",
    "                <td style=\"padding: 10px; text-align: center;\"><strong>0.7 - 0.9</strong></td>\n",
    "                <td style=\"padding: 10px;\">Needs engagement and variety</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 10px;\">Creative Taglines</td>\n",
    "                <td style=\"padding: 10px; text-align: center;\"><strong>0.9 - 1.0</strong></td>\n",
    "                <td style=\"padding: 10px;\">Maximize uniqueness and creativity</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        \n",
    "        <h3 style=\"margin-top: 25px;\">‚ö†Ô∏è Best Practices</h3>\n",
    "        <ul>\n",
    "            <li><strong>Test different temperatures:</strong> Run the same prompt with 0.3, 0.7, and 1.0 to see differences</li>\n",
    "            <li><strong>Combine with top_p:</strong> Temperature and top_p work together. Both at 1.0 = maximum creativity</li>\n",
    "            <li><strong>Lower for facts:</strong> Anything requiring accuracy should use temperature ‚â§ 0.3</li>\n",
    "            <li><strong>Higher for creativity:</strong> Marketing, brainstorming, and unique content benefit from ‚â• 0.7</li>\n",
    "            <li><strong>Document your settings:</strong> Keep track of what temperature works best for each content type</li>\n",
    "        </ul>\n",
    "        \n",
    "        <p style=\"margin-top: 20px;\"><strong>Generated:</strong> \"\"\" + time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"\"\"</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    \n",
    "    # Save HTML to file\n",
    "    output_file = \"temperature_comparison_chatgpt.html\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_output)\n",
    "    \n",
    "    print(f\"\\n‚úÖ HTML table saved to: {output_file}\")\n",
    "    print(\"üìÑ Open the file in your browser to view the formatted results!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Alternative: Console-only output (matches your original format)\n",
    "def temperature_comparison_console():\n",
    "    \"\"\"Compare different temperature settings with console output only\"\"\"\n",
    "    print(\"üå°Ô∏è  Temperature Parameter Comparison\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    base_prompt = (\n",
    "        \"Write about FUJIFILM Apeos Printers. \"\n",
    "        \"Make it 2 paragraphs.\"\n",
    "    )\n",
    "\n",
    "    temperatures = [0.0, 0.3, 0.7, 1.0]\n",
    "\n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nüå°Ô∏è  TEMPERATURE: {temp}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        try:\n",
    "            # ChatGPT API with temperature control\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": base_prompt}\n",
    "                ],\n",
    "                temperature=temp,\n",
    "                max_tokens=200\n",
    "            )\n",
    "\n",
    "            output_text = response.choices[0].message.content\n",
    "\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output_text)\n",
    "\n",
    "            print(f\"\\nüìä Response Length: {len(output_text)} characters\")\n",
    "            print(f\"üìä Word Count: {len(output_text.split())} words\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Choose output format:\")\n",
    "    print(\"1. HTML Table (saved to file)\")\n",
    "    print(\"2. Console Output Only\")\n",
    "    \n",
    "    choice = input(\"\\nEnter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        results = temperature_comparison_html()\n",
    "    else:\n",
    "        temperature_comparison_console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_comparison():\n",
    "    \"\"\"Compare ChatGPT responses using different temperature settings and output as HTML table.\"\"\"\n",
    "\n",
    "    base_prompt = \"Write about FUJIFILM Apeos Printers. Make it 2 paragraphs.\"\n",
    "    temperatures = [0.0, 0.3, 0.7, 1.0]\n",
    "\n",
    "    # Start HTML table\n",
    "    html = \"\"\"\n",
    "    <table border=\"1\" cellpadding=\"8\" style=\"border-collapse: collapse; width: 100%;\">\n",
    "        <tr style=\"background-color: #f2f2f2; font-weight: bold;\">\n",
    "            <th>Temperature</th>\n",
    "            <th>Response</th>\n",
    "            <th>Word Count</th>\n",
    "            <th>Character Count</th>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "\n",
    "    for temp in temperatures:\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=MODEL,\n",
    "                input=base_prompt,\n",
    "                temperature=temp,\n",
    "                max_output_tokens=30\n",
    "            )\n",
    "\n",
    "            text = response.output_text\n",
    "            word_count = len(text.split())\n",
    "            char_count = len(text)\n",
    "\n",
    "            html += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{temp}</td>\n",
    "                <td>{text}</td>\n",
    "                <td>{word_count}</td>\n",
    "                <td>{char_count}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            html += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{temp}</td>\n",
    "                <td>Error: {e}</td>\n",
    "                <td>-</td>\n",
    "                <td>-</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "\n",
    "    html += \"</table>\"\n",
    "\n",
    "    print(html)   # Print HTML so you can copy or render it\n",
    "\n",
    "    return html   # Function returns the HTML table for embedding\n",
    "\n",
    "\n",
    "# Run the test\n",
    "temperature_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e35ee",
   "metadata": {},
   "source": [
    "## Top-P (Nucleus Sampling) Demonstration\n",
    "Test various top-p values (0.1, 0.5, 0.8, 0.95) to demonstrate how nucleus sampling affects response diversity and focus.\n",
    "- **Low top-p (0.1)**: Very focused, conservative responses\n",
    "- **High top-p (0.95)**: More diverse, varied vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45afb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def top_p_comparison():\n",
    "    \"\"\"Compare different top-p values using ChatGPT\"\"\"\n",
    "    print(\"üéØ Top-P (Nucleus Sampling) Comparison\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    base_prompt = (\n",
    "        \"Explain quantum computing in simple terms. \"\n",
    "        \"Include 3 key concepts and their practical applications.\"\n",
    "    )\n",
    "\n",
    "    top_p_values = [0.1, 0.5, 0.8, 0.95]\n",
    "\n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    for top_p in top_p_values:\n",
    "        print(f\"\\nüéØ TOP-P: {top_p}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        try:\n",
    "            # ChatGPT Responses API with top_p sampling\n",
    "            response = client.responses.create(\n",
    "                model=MODEL,\n",
    "                input=base_prompt,\n",
    "                temperature=0.7,        # constant for fairness\n",
    "                top_p=top_p,\n",
    "                max_output_tokens=40\n",
    "            )\n",
    "\n",
    "            output = response.output_text\n",
    "\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output)\n",
    "\n",
    "            print(f\"\\nüìä Response Length: {len(output)} characters\")\n",
    "            print(f\"üìä Unique Words: {len(set(output.lower().split()))} words\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "top_p_comparison()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#from openai import OpenAI\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "MODEL = \"gpt-4\"\n",
    "\n",
    "def top_p_comparison_html():\n",
    "    \"\"\"Compare different top-p values using ChatGPT and output as HTML table\"\"\"\n",
    "    \n",
    "    print(\"üéØ Top-P (Nucleus Sampling) Comparison - Fujifilm Instax Mini 12\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    base_prompt = (\n",
    "        \"Write a 40-word product tagline for Fujifilm Instax Mini 12. \"\n",
    "        \"Make it engaging and suitable for marketing.\"\n",
    "    )\n",
    "    \n",
    "    top_p_values = [0.1, 0.5, 0.8, 0.95]\n",
    "    results = []\n",
    "    \n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Generate responses for each top_p value\n",
    "    for top_p in top_p_values:\n",
    "        print(f\"\\nüîÑ Generating with top_p={top_p}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # OpenAI Chat Completion API with top_p sampling\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": base_prompt}\n",
    "                ],\n",
    "                temperature=0.7,        # Keep constant for fair comparison\n",
    "                top_p=top_p,           # Variable we're testing\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            output_text = response.choices[0].message.content\n",
    "            word_count = len(output_text.split())\n",
    "            unique_words = len(set(output_text.lower().split()))\n",
    "            char_length = len(output_text)\n",
    "            \n",
    "            # Determine characteristics based on top_p value\n",
    "            if top_p <= 0.3:\n",
    "                characteristics = \"Predictable, safe words, similar across runs\"\n",
    "                category = \"Focused\"\n",
    "            elif top_p <= 0.6:\n",
    "                characteristics = \"More varied language, still coherent\"\n",
    "                category = \"Balanced\"\n",
    "            elif top_p <= 0.85:\n",
    "                characteristics = \"Creative word choices, unique phrasing\"\n",
    "                category = \"Creative\"\n",
    "            else:\n",
    "                characteristics = \"Unexpected words, more experimental\"\n",
    "                category = \"Wide Open\"\n",
    "            \n",
    "            results.append({\n",
    "                'top_p': top_p,\n",
    "                'category': category,\n",
    "                'output': output_text,\n",
    "                'characteristics': characteristics,\n",
    "                'word_count': word_count,\n",
    "                'unique_words': unique_words,\n",
    "                'char_length': char_length\n",
    "            })\n",
    "            \n",
    "            print(f\"üìù Generated Response:\")\n",
    "            print(f\"{output_text}\\n\")\n",
    "            print(f\"üìä Response Length: {char_length} characters\")\n",
    "            print(f\"üìä Word Count: {word_count} words\")\n",
    "            print(f\"üìä Unique Words: {unique_words} words\")\n",
    "            print(f\"‚úÖ Success!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with top_p={top_p}: {e}\")\n",
    "            results.append({\n",
    "                'top_p': top_p,\n",
    "                'category': 'Error',\n",
    "                'output': f'Error: {str(e)}',\n",
    "                'characteristics': 'N/A',\n",
    "                'word_count': 0,\n",
    "                'unique_words': 0,\n",
    "                'char_length': 0\n",
    "            })\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    # Generate HTML table\n",
    "    html_output = generate_html_table(results, base_prompt)\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = \"top_p_comparison_results.html\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_output)\n",
    "    \n",
    "    print(f\"\\n‚úÖ HTML table saved to: {output_file}\")\n",
    "    print(\"üìÑ Open the file in your browser to view formatted results!\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    return html_output\n",
    "\n",
    "\n",
    "def generate_html_table(results, prompt):\n",
    "    \"\"\"Generate formatted HTML table from results\"\"\"\n",
    "    \n",
    "    # Color mapping for different top_p ranges\n",
    "    color_map = {\n",
    "        0.1: {'bg': '#fff3e0', 'text': '#e65100'},\n",
    "        0.5: {'bg': '#e8f5e9', 'text': '#2e7d32'},\n",
    "        0.8: {'bg': '#f3e5f5', 'text': '#6a1b9a'},\n",
    "        0.95: {'bg': '#fce4ec', 'text': '#c2185b'}\n",
    "    }\n",
    "    \n",
    "    html = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Top-P Comparison Results - Fujifilm Instax Mini 12</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            max-width: 1400px;\n",
    "            margin: 40px auto;\n",
    "            padding: 20px;\n",
    "            background: #f5f5f5;\n",
    "        }}\n",
    "        \n",
    "        .header {{\n",
    "            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);\n",
    "            color: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 10px;\n",
    "            margin-bottom: 30px;\n",
    "        }}\n",
    "        \n",
    "        .header h1 {{\n",
    "            margin: 0;\n",
    "            font-size: 32px;\n",
    "        }}\n",
    "        \n",
    "        .fujifilm-logo {{\n",
    "            color: #e60012;\n",
    "            font-weight: bold;\n",
    "            font-size: 24px;\n",
    "            margin-bottom: 10px;\n",
    "        }}\n",
    "        \n",
    "        .prompt-box {{\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 30px;\n",
    "            border-left: 4px solid #e60012;\n",
    "        }}\n",
    "        \n",
    "        .prompt-box strong {{\n",
    "            color: #e60012;\n",
    "        }}\n",
    "        \n",
    "        table {{\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            background: white;\n",
    "            box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
    "            border-radius: 8px;\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        \n",
    "        thead {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "        }}\n",
    "        \n",
    "        th {{\n",
    "            padding: 15px;\n",
    "            text-align: left;\n",
    "            font-weight: 600;\n",
    "            font-size: 14px;\n",
    "        }}\n",
    "        \n",
    "        td {{\n",
    "            padding: 15px;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "            font-size: 14px;\n",
    "            vertical-align: top;\n",
    "        }}\n",
    "        \n",
    "        tr:last-child td {{\n",
    "            border-bottom: none;\n",
    "        }}\n",
    "        \n",
    "        tr:hover {{\n",
    "            background: rgba(0,0,0,0.02);\n",
    "        }}\n",
    "        \n",
    "        .top-p-cell {{\n",
    "            font-weight: bold;\n",
    "            font-size: 18px;\n",
    "        }}\n",
    "        \n",
    "        .category {{\n",
    "            font-size: 12px;\n",
    "            font-weight: normal;\n",
    "            display: block;\n",
    "            margin-top: 5px;\n",
    "            opacity: 0.8;\n",
    "        }}\n",
    "        \n",
    "        .output-text {{\n",
    "            font-style: italic;\n",
    "            line-height: 1.6;\n",
    "            color: #333;\n",
    "        }}\n",
    "        \n",
    "        .stats {{\n",
    "            text-align: center;\n",
    "            font-weight: 600;\n",
    "            font-size: 16px;\n",
    "        }}\n",
    "        \n",
    "        .footer {{\n",
    "            margin-top: 30px;\n",
    "            padding: 20px;\n",
    "            background: white;\n",
    "            border-radius: 8px;\n",
    "            border-left: 4px solid #4caf50;\n",
    "        }}\n",
    "        \n",
    "        .footer h3 {{\n",
    "            margin-top: 0;\n",
    "            color: #2e7d32;\n",
    "        }}\n",
    "        \n",
    "        .footer ul {{\n",
    "            line-height: 1.8;\n",
    "        }}\n",
    "        \n",
    "        .summary-stats {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "            gap: 15px;\n",
    "            margin-bottom: 30px;\n",
    "        }}\n",
    "        \n",
    "        .stat-card {{\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "            text-align: center;\n",
    "        }}\n",
    "        \n",
    "        .stat-card h3 {{\n",
    "            margin: 0;\n",
    "            color: #666;\n",
    "            font-size: 14px;\n",
    "            text-transform: uppercase;\n",
    "        }}\n",
    "        \n",
    "        .stat-card p {{\n",
    "            margin: 10px 0 0 0;\n",
    "            font-size: 32px;\n",
    "            font-weight: bold;\n",
    "            color: #1e3c72;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <div class=\"fujifilm-logo\">FUJIFILM</div>\n",
    "        <h1>üéØ Top-P (Nucleus Sampling) Comparison</h1>\n",
    "        <p style=\"margin: 10px 0 0 0; font-size: 18px;\">Instax Mini 12 Tagline Generation Analysis</p>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"prompt-box\">\n",
    "        <strong>üìã Base Prompt:</strong> {prompt}\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"summary-stats\">\n",
    "        <div class=\"stat-card\">\n",
    "            <h3>Tests Run</h3>\n",
    "            <p>{len(results)}</p>\n",
    "        </div>\n",
    "        <div class=\"stat-card\">\n",
    "            <h3>Avg Word Count</h3>\n",
    "            <p>{sum(r['word_count'] for r in results) // len(results)}</p>\n",
    "        </div>\n",
    "        <div class=\"stat-card\">\n",
    "            <h3>Temperature</h3>\n",
    "            <p>0.7</p>\n",
    "        </div>\n",
    "        <div class=\"stat-card\">\n",
    "            <h3>Model</h3>\n",
    "            <p style=\"font-size: 20px;\">GPT-4</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th style=\"width: 12%;\">top_p Value</th>\n",
    "                <th style=\"width: 42%;\">Generated Tagline</th>\n",
    "                <th style=\"width: 26%;\">Characteristics</th>\n",
    "                <th style=\"width: 8%; text-align: center;\">Words</th>\n",
    "                <th style=\"width: 7%; text-align: center;\">Unique</th>\n",
    "                <th style=\"width: 5%; text-align: center;\">Chars</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "\"\"\"\n",
    "    \n",
    "    # Add rows for each result\n",
    "    for result in results:\n",
    "        top_p = result['top_p']\n",
    "        colors = color_map.get(top_p, {'bg': '#ffffff', 'text': '#000000'})\n",
    "        \n",
    "        html += f\"\"\"\n",
    "            <tr style=\"background: {colors['bg']};\">\n",
    "                <td class=\"top-p-cell\" style=\"color: {colors['text']};\">\n",
    "                    {top_p}\n",
    "                    <span class=\"category\">({result['category']})</span>\n",
    "                </td>\n",
    "                <td class=\"output-text\">\"{result['output']}\"</td>\n",
    "                <td>{result['characteristics']}</td>\n",
    "                <td class=\"stats\">{result['word_count']}</td>\n",
    "                <td class=\"stats\">{result['unique_words']}</td>\n",
    "                <td class=\"stats\" style=\"font-size: 12px;\">{result['char_length']}</td>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "        </tbody>\n",
    "    </table>\n",
    "    \n",
    "    <div class=\"footer\">\n",
    "        <h3>üìä Key Insights About top_p Parameter</h3>\n",
    "        <ul>\n",
    "            <li><strong>top_p = 0.1 (Focused):</strong> Best for technical documentation, specifications, and consistent outputs. Uses only the most probable words.</li>\n",
    "            <li><strong>top_p = 0.5 (Balanced):</strong> Balanced choice for product descriptions, emails, and general business content. Good mix of consistency and variety.</li>\n",
    "            <li><strong>top_p = 0.8 (Creative):</strong> Ideal for creative marketing copy, brainstorming, and varied content. More linguistic diversity.</li>\n",
    "            <li><strong>top_p = 0.95 (Wide Open):</strong> Most creative and experimental, good for taglines and unique content. Maximum vocabulary range.</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3 style=\"margin-top: 25px;\">üî¨ Technical Details</h3>\n",
    "        <ul>\n",
    "            <li><strong>Temperature:</strong> Held constant at 0.7 for all tests to isolate the effect of top_p</li>\n",
    "            <li><strong>Model:</strong> GPT-4 (OpenAI)</li>\n",
    "            <li><strong>top_p Definition:</strong> Nucleus sampling - limits token selection to smallest set with cumulative probability ‚â• top_p</li>\n",
    "            <li><strong>Use Case:</strong> Fujifilm Instax Mini 12 marketing tagline generation</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3 style=\"margin-top: 25px;\">üí° Recommendations for Fujifilm Content</h3>\n",
    "        <ul>\n",
    "            <li><strong>Product Specs/Documentation:</strong> Use top_p = 0.1-0.3</li>\n",
    "            <li><strong>Product Descriptions:</strong> Use top_p = 0.5-0.7</li>\n",
    "            <li><strong>Social Media Posts:</strong> Use top_p = 0.7-0.8</li>\n",
    "            <li><strong>Creative Taglines:</strong> Use top_p = 0.8-0.95</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"text-align: center; margin-top: 30px; padding: 20px; background: white; border-radius: 8px;\">\n",
    "        <p style=\"color: #666; font-size: 12px; margin: 0;\">\n",
    "            <strong>Generated on:</strong> \"\"\" + time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"\"\"<br>\n",
    "            <strong>API:</strong> OpenAI GPT-4 | <strong>Framework:</strong> OpenAI Python Client\n",
    "        </p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "\n",
    "# Alternative function for console-only output (your original format)\n",
    "def top_p_comparison_console():\n",
    "    \"\"\"Compare different top-p values with console output only\"\"\"\n",
    "    print(\"üéØ Top-P (Nucleus Sampling) Comparison\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    base_prompt = (\n",
    "        \"Write a 40-word product tagline for Fujifilm Instax Mini 12. \"\n",
    "        \"Make it engaging and suitable for marketing.\"\n",
    "    )\n",
    "\n",
    "    top_p_values = [0.1, 0.5, 0.8, 0.95]\n",
    "\n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    for top_p in top_p_values:\n",
    "        print(f\"\\nüéØ TOP-P: {top_p}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        try:\n",
    "            # OpenAI Chat API with top_p sampling\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": base_prompt}\n",
    "                ],\n",
    "                temperature=0.7,        # constant for fairness\n",
    "                top_p=top_p,\n",
    "                max_tokens=100\n",
    "            )\n",
    "\n",
    "            output = response.choices[0].message.content\n",
    "\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output)\n",
    "\n",
    "            print(f\"\\nüìä Response Length: {len(output)} characters\")\n",
    "            print(f\"üìä Unique Words: {len(set(output.lower().split()))} words\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Choose output format:\")\n",
    "    print(\"1. HTML Table (saved to file)\")\n",
    "    print(\"2. Console Output Only\")\n",
    "    \n",
    "    choice = input(\"\\nEnter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        html_result = top_p_comparison_html()\n",
    "    else:\n",
    "        top_p_comparison_console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c7dd7",
   "metadata": {},
   "source": [
    "## Max Output Tokens Control\n",
    "Demonstrate how max_output_tokens parameter (50, 150, 300, 500) controls response length and content completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1675b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def max_tokens_comparison():\n",
    "    \"\"\"Demonstrate max_output_tokens parameter using ChatGPT\"\"\"\n",
    "    print(\"üìè Max Output Tokens Comparison\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    base_prompt = (\n",
    "        \"Explain the history of the internet, from its origins to modern day. \"\n",
    "        \"Include major milestones and key technologies.\"\n",
    "    )\n",
    "\n",
    "    token_limits = [50, 150, 300, 500]\n",
    "\n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    for max_tokens in token_limits:\n",
    "        print(f\"\\nüìè MAX OUTPUT TOKENS: {max_tokens}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        try:\n",
    "            # ChatGPT Responses API with different token limits\n",
    "            response = client.responses.create(\n",
    "                model=MODEL,     # reliable model\n",
    "                input=base_prompt,\n",
    "                temperature=0.5,\n",
    "                \n",
    "                max_output_tokens=max_tokens\n",
    "            )\n",
    "\n",
    "            output = response.output_text\n",
    "\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output)\n",
    "\n",
    "            print(f\"\\nüìä Actual Length: {len(output)} characters\")\n",
    "            print(f\"üìä Word Count: {len(output.split())} words\")\n",
    "            print(f\"üìä Estimated Tokens: ~{len(output.split()) * 1.3:.0f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Run it\n",
    "max_tokens_comparison()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def max_tokens_comparison_html():\n",
    "    \"\"\"Demonstrate max_tokens parameter using ChatGPT and output as HTML table\"\"\"\n",
    "    \n",
    "    print(\"üìè Max Output Tokens Comparison\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Base prompt - Fujifilm related\n",
    "    base_prompt = (\n",
    "        \"Explain the history and evolution of Fujifilm's Instax instant camera line, \"\n",
    "        \"from its origins to modern day. Include major milestones, key technologies, \"\n",
    "        \"and how it revolutionized instant photography.\"\n",
    "    )\n",
    "    \n",
    "    token_limits = [50, 150, 300, 500]\n",
    "    \n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Start building HTML table\n",
    "    html_output = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Max Tokens Comparison - ChatGPT</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            max-width: 1600px;\n",
    "            margin: 40px auto;\n",
    "            padding: 20px;\n",
    "            background: #f5f5f5;\n",
    "        }\n",
    "        .header {\n",
    "            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);\n",
    "            color: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 10px;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .header h1 {\n",
    "            margin: 0;\n",
    "            font-size: 32px;\n",
    "        }\n",
    "        .fujifilm-logo {\n",
    "            color: #e60012;\n",
    "            font-weight: bold;\n",
    "            font-size: 24px;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        .prompt-box {\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 30px;\n",
    "            border-left: 4px solid #e60012;\n",
    "        }\n",
    "        table {\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            background: white;\n",
    "            box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        thead {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "        }\n",
    "        th {\n",
    "            padding: 15px;\n",
    "            text-align: left;\n",
    "            font-weight: 600;\n",
    "        }\n",
    "        td {\n",
    "            padding: 15px;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "            vertical-align: top;\n",
    "        }\n",
    "        tr:hover {\n",
    "            background: rgba(0,0,0,0.02);\n",
    "        }\n",
    "        .token-50 { background: #ffebee; }\n",
    "        .token-150 { background: #fff3e0; }\n",
    "        .token-300 { background: #e8f5e9; }\n",
    "        .token-500 { background: #e3f2fd; }\n",
    "        .max-tokens-cell {\n",
    "            font-weight: bold;\n",
    "            font-size: 20px;\n",
    "            color: #1e3c72;\n",
    "        }\n",
    "        .output-text {\n",
    "            line-height: 1.6;\n",
    "            font-size: 14px;\n",
    "            max-width: 600px;\n",
    "        }\n",
    "        .stats {\n",
    "            text-align: center;\n",
    "            font-weight: 600;\n",
    "            font-size: 16px;\n",
    "        }\n",
    "        .truncated {\n",
    "            color: #d32f2f;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        .complete {\n",
    "            color: #388e3c;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        .chart-container {\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .bar {\n",
    "            height: 30px;\n",
    "            margin: 10px 0;\n",
    "            border-radius: 5px;\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            padding-left: 10px;\n",
    "            color: white;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        .info-box {\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            border-left: 4px solid #4caf50;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <div class=\"fujifilm-logo\">FUJIFILM</div>\n",
    "        <h1>üìè Max Tokens Comparison - ChatGPT (GPT-4)</h1>\n",
    "        <p style=\"margin: 10px 0 0 0; font-size: 18px;\">Understanding Token Limits in AI Generation</p>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"prompt-box\">\n",
    "        <strong>üìã Base Prompt:</strong> \"\"\" + base_prompt + \"\"\"\n",
    "    </div>\n",
    "    \n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th style=\"width: 10%;\">Max Tokens</th>\n",
    "                <th style=\"width: 45%;\">Generated Response</th>\n",
    "                <th style=\"width: 10%; text-align: center;\">Characters</th>\n",
    "                <th style=\"width: 10%; text-align: center;\">Words</th>\n",
    "                <th style=\"width: 10%; text-align: center;\">Est. Tokens</th>\n",
    "                <th style=\"width: 15%; text-align: center;\">Status</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Generate responses for each token limit\n",
    "    for max_tokens in token_limits:\n",
    "        print(f\"\\nüìè MAX OUTPUT TOKENS: {max_tokens}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # ChatGPT API call with token limit\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": base_prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            output = response.choices[0].message.content\n",
    "            finish_reason = response.choices[0].finish_reason\n",
    "            \n",
    "            # Calculate statistics\n",
    "            char_length = len(output)\n",
    "            word_count = len(output.split())\n",
    "            estimated_tokens = int(word_count * 1.3)  # Rough estimate\n",
    "            \n",
    "            # Determine if response was truncated\n",
    "            if finish_reason == \"length\":\n",
    "                status = \"TRUNCATED\"\n",
    "                status_class = \"truncated\"\n",
    "            else:\n",
    "                status = \"COMPLETE\"\n",
    "                status_class = \"complete\"\n",
    "            \n",
    "            # Determine row class\n",
    "            row_class = f\"token-{max_tokens}\"\n",
    "            \n",
    "            # Print to console\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output)\n",
    "            print(f\"\\nüìä Actual Length: {char_length} characters\")\n",
    "            print(f\"üìä Word Count: {word_count} words\")\n",
    "            print(f\"üìä Estimated Tokens: ~{estimated_tokens}\")\n",
    "            print(f\"üìä Finish Reason: {finish_reason}\")\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'max_tokens': max_tokens,\n",
    "                'output': output,\n",
    "                'char_length': char_length,\n",
    "                'word_count': word_count,\n",
    "                'estimated_tokens': estimated_tokens,\n",
    "                'status': status\n",
    "            })\n",
    "            \n",
    "            # Add row to HTML table\n",
    "            html_output += f\"\"\"\n",
    "            <tr class=\"{row_class}\">\n",
    "                <td class=\"max-tokens-cell\">{max_tokens}</td>\n",
    "                <td class=\"output-text\">{output}</td>\n",
    "                <td class=\"stats\">{char_length}</td>\n",
    "                <td class=\"stats\">{word_count}</td>\n",
    "                <td class=\"stats\">{estimated_tokens}</td>\n",
    "                <td class=\"stats {status_class}\">{status}</td>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            html_output += f\"\"\"\n",
    "            <tr>\n",
    "                <td class=\"max-tokens-cell\">{max_tokens}</td>\n",
    "                <td colspan=\"5\" style=\"color: red;\">Error: {str(e)}</td>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    # Close table\n",
    "    html_output += \"\"\"\n",
    "        </tbody>\n",
    "    </table>\n",
    "\"\"\"\n",
    "    \n",
    "    # Add visual comparison chart\n",
    "    if results:\n",
    "        html_output += \"\"\"\n",
    "    <div class=\"chart-container\">\n",
    "        <h3>üìä Visual Comparison - Word Count by Token Limit</h3>\n",
    "\"\"\"\n",
    "        max_words = max(r['word_count'] for r in results)\n",
    "        colors = ['#f44336', '#ff9800', '#4caf50', '#2196f3']\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            width_percent = (result['word_count'] / max_words * 100) if max_words > 0 else 0\n",
    "            html_output += f\"\"\"\n",
    "        <div class=\"bar\" style=\"width: {width_percent}%; background: {colors[i]};\">\n",
    "            {result['max_tokens']} tokens ‚Üí {result['word_count']} words\n",
    "        </div>\n",
    "\"\"\"\n",
    "        \n",
    "        html_output += \"\"\"\n",
    "    </div>\n",
    "\"\"\"\n",
    "    \n",
    "    # Add insights section\n",
    "    html_output += \"\"\"\n",
    "    <div class=\"info-box\">\n",
    "        <h3>üí° Key Insights About max_tokens Parameter</h3>\n",
    "        <ul>\n",
    "            <li><strong>What is max_tokens?</strong> The maximum number of tokens (word pieces) the AI can generate in a response.</li>\n",
    "            <li><strong>Rough Estimate:</strong> 1 token ‚âà 0.75 words or 1 word ‚âà 1.3 tokens (varies by language and complexity).</li>\n",
    "            <li><strong>Truncation:</strong> When max_tokens is reached, the response stops mid-sentence (status: TRUNCATED).</li>\n",
    "            <li><strong>Complete Responses:</strong> If the AI finishes naturally before hitting the limit (status: COMPLETE).</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3 style=\"margin-top: 25px;\">üéØ Recommended Token Limits for Fujifilm Content</h3>\n",
    "        <ul>\n",
    "            <li><strong>50-100 tokens:</strong> Short taglines, product names, brief descriptions</li>\n",
    "            <li><strong>150-250 tokens:</strong> Product descriptions, social media posts, short emails</li>\n",
    "            <li><strong>300-500 tokens:</strong> Detailed product specs, blog post introductions, press releases</li>\n",
    "            <li><strong>500-1000 tokens:</strong> Technical documentation, full articles, comprehensive guides</li>\n",
    "            <li><strong>1000+ tokens:</strong> Long-form content, white papers, detailed tutorials</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3 style=\"margin-top: 25px;\">‚ö†Ô∏è Best Practices</h3>\n",
    "        <ul>\n",
    "            <li>Always set max_tokens to prevent runaway generation and control costs</li>\n",
    "            <li>Set it higher than needed to avoid truncation (you can always use less)</li>\n",
    "            <li>Monitor the finish_reason in API responses to detect truncation</li>\n",
    "            <li>For critical content, add 20-30% buffer to your estimated token needs</li>\n",
    "        </ul>\n",
    "        \n",
    "        <p style=\"margin-top: 20px;\"><strong>Generated:</strong> \"\"\" + time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"\"\"</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    \n",
    "    # Save HTML to file\n",
    "    output_file = \"max_tokens_comparison_chatgpt.html\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_output)\n",
    "    \n",
    "    print(f\"\\n‚úÖ HTML table saved to: {output_file}\")\n",
    "    print(\"üìÑ Open the file in your browser to view the formatted results!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Alternative: Console-only output (matches your original format)\n",
    "def max_tokens_comparison_console():\n",
    "    \"\"\"Demonstrate max_tokens parameter with console output only\"\"\"\n",
    "    print(\"üìè Max Output Tokens Comparison\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    base_prompt = (\n",
    "        \"Explain the history and evolution of Fujifilm's Instax instant camera line. \"\n",
    "        \"Include major milestones and key technologies.\"\n",
    "    )\n",
    "\n",
    "    token_limits = [50, 150, 300, 500]\n",
    "\n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    for max_tokens in token_limits:\n",
    "        print(f\"\\nüìè MAX OUTPUT TOKENS: {max_tokens}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        try:\n",
    "            # ChatGPT API with different token limits\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": base_prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "\n",
    "            output = response.choices[0].message.content\n",
    "\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output)\n",
    "\n",
    "            print(f\"\\nüìä Actual Length: {len(output)} characters\")\n",
    "            print(f\"üìä Word Count: {len(output.split())} words\")\n",
    "            print(f\"üìä Estimated Tokens: ~{len(output.split()) * 1.3:.0f}\")\n",
    "            print(f\"üìä Finish Reason: {response.choices[0].finish_reason}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Choose output format:\")\n",
    "    print(\"1. HTML Table (saved to file)\")\n",
    "    print(\"2. Console Output Only\")\n",
    "    \n",
    "    choice = input(\"\\nEnter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        results = max_tokens_comparison_html()\n",
    "    else:\n",
    "        max_tokens_comparison_console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9a03c",
   "metadata": {},
   "source": [
    "## Stop Sequences Demonstration\n",
    "Show how stop sequences can control when generation ends, useful for structured outputs and preventing unwanted content continuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Replace the old stop_sequences_demo cell with the code below)\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def _truncate_on_stops(text: str, stops):\n",
    "    if not stops:\n",
    "        return text\n",
    "    earliest = None\n",
    "    for s in stops:\n",
    "        idx = text.find(s)\n",
    "        if idx != -1:\n",
    "            if earliest is None or idx < earliest:\n",
    "                earliest = idx\n",
    "    return text if earliest is None else text[:earliest]\n",
    "\n",
    "def stop_sequences_demo():\n",
    "    \"\"\"Demonstrate stop-sequence behavior without using unsupported 'stop' param.\"\"\"\n",
    "    print(\"üõë Stop Sequences Demonstration (simulated client-side)\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    scenarios = [\n",
    "        {\"name\": \"No Stop Sequences\", \"prompt\": \"List the planets in our solar system with descriptions:\", \"stop_sequences\": None},\n",
    "        {\"name\": \"Stop at numbered list item\", \"prompt\": \"List the planets in our solar system with descriptions:\", \"stop_sequences\": [\"5.\"]},\n",
    "        {\"name\": \"Stop at specific word\", \"prompt\": \"Write a story about space exploration. The story should mention Mars.\", \"stop_sequences\": [\"Mars\"]},\n",
    "        {\"name\": \"Multiple stop sequences\", \"prompt\": \"Explain programming concepts: variables, functions, loops, and classes.\", \"stop_sequences\": [\"loops\", \"classes\"]},\n",
    "    ]\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\nüõë SCENARIO: {scenario['name']}\")\n",
    "        print(f\"üéØ Prompt: {scenario['prompt']}\")\n",
    "        print(f\"üõë Stop Sequences: {scenario['stop_sequences']}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        try:\n",
    "            # Call API WITHOUT 'stop' (avoid unsupported kwarg)\n",
    "            response = client.responses.create(\n",
    "                model=MODEL,\n",
    "                input=scenario[\"prompt\"],\n",
    "                temperature=0.7,\n",
    "                max_output_tokens=400\n",
    "            )\n",
    "\n",
    "            output = response.output_text\n",
    "            # Truncate locally based on provided stop sequences\n",
    "            final_output = _truncate_on_stops(output, scenario[\"stop_sequences\"])\n",
    "\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(final_output)\n",
    "            print(f\"\\nüìä Response Length: {len(final_output)} characters\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)\n",
    "\n",
    "# Run the stop-sequence test\n",
    "stop_sequences_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def stop_sequences_demo_html():\n",
    "    \"\"\"Demonstrate stop sequences with OpenAI API and output as HTML table\"\"\"\n",
    "    \n",
    "    print(\"üõë Stop Sequences Demonstration\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Define test scenarios with Fujifilm-related content\n",
    "    scenarios = [\n",
    "        {\n",
    "            \"name\": \"No Stop Sequences\",\n",
    "            \"prompt\": \"List the key features of FUJIFILM Apeos C325 printer with descriptions:\",\n",
    "            \"stop_sequences\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Stop at List Item\",\n",
    "            \"prompt\": \"List the key features of FUJIFILM Apeos C325 printer with descriptions:\",\n",
    "            \"stop_sequences\": [\"4.\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Stop at Specific Word\",\n",
    "            \"prompt\": \"Write about FUJIFILM Instax instant cameras. Mention the different models including Mini, Square, and Wide formats.\",\n",
    "            \"stop_sequences\": [\"Square\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Multiple Stop Sequences\",\n",
    "            \"prompt\": \"Explain FUJIFILM product lines: cameras, printers, medical imaging, and optical devices.\",\n",
    "            \"stop_sequences\": [\"medical\", \"optical\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Stop at Section Marker\",\n",
    "            \"prompt\": \"Write a product description for FUJIFILM Instax Mini 12. Include sections: Features, Benefits, and Target Audience.\",\n",
    "            \"stop_sequences\": [\"Benefits:\", \"Target\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Start building HTML\n",
    "    html_output = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Stop Sequences Demo - OpenAI GPT-4</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            max-width: 1600px;\n",
    "            margin: 40px auto;\n",
    "            padding: 20px;\n",
    "            background: #f5f5f5;\n",
    "        }\n",
    "        .header {\n",
    "            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);\n",
    "            color: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 10px;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .header h1 {\n",
    "            margin: 0;\n",
    "            font-size: 32px;\n",
    "        }\n",
    "        .fujifilm-logo {\n",
    "            color: #e60012;\n",
    "            font-weight: bold;\n",
    "            font-size: 24px;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        .info-banner {\n",
    "            background: #fff3cd;\n",
    "            border-left: 4px solid #ffc107;\n",
    "            padding: 15px;\n",
    "            margin-bottom: 30px;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        table {\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            background: white;\n",
    "            box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        thead {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "        }\n",
    "        th {\n",
    "            padding: 15px;\n",
    "            text-align: left;\n",
    "            font-weight: 600;\n",
    "            font-size: 14px;\n",
    "        }\n",
    "        td {\n",
    "            padding: 15px;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "            vertical-align: top;\n",
    "        }\n",
    "        tr:hover {\n",
    "            background: rgba(0,0,0,0.02);\n",
    "        }\n",
    "        .scenario-name {\n",
    "            font-weight: bold;\n",
    "            font-size: 16px;\n",
    "            color: #1e3c72;\n",
    "        }\n",
    "        .prompt-text {\n",
    "            font-size: 13px;\n",
    "            color: #666;\n",
    "            font-style: italic;\n",
    "            margin-top: 5px;\n",
    "        }\n",
    "        .stop-sequences {\n",
    "            background: #ffebee;\n",
    "            padding: 8px;\n",
    "            border-radius: 5px;\n",
    "            font-family: 'Courier New', monospace;\n",
    "            font-size: 12px;\n",
    "            color: #c62828;\n",
    "        }\n",
    "        .no-stop {\n",
    "            background: #e8f5e9;\n",
    "            color: #2e7d32;\n",
    "        }\n",
    "        .output-text {\n",
    "            line-height: 1.6;\n",
    "            font-size: 14px;\n",
    "            max-height: 200px;\n",
    "            overflow-y: auto;\n",
    "            background: #f9f9f9;\n",
    "            padding: 10px;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        .stats {\n",
    "            text-align: center;\n",
    "            font-weight: 600;\n",
    "            font-size: 14px;\n",
    "        }\n",
    "        .truncated-indicator {\n",
    "            background: #f44336;\n",
    "            color: white;\n",
    "            padding: 3px 8px;\n",
    "            border-radius: 3px;\n",
    "            font-size: 11px;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        .complete-indicator {\n",
    "            background: #4caf50;\n",
    "            color: white;\n",
    "            padding: 3px 8px;\n",
    "            border-radius: 3px;\n",
    "            font-size: 11px;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        .info-box {\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            border-left: 4px solid #4caf50;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .comparison-section {\n",
    "            display: grid;\n",
    "            grid-template-columns: 1fr 1fr;\n",
    "            gap: 20px;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .comparison-card {\n",
    "            background: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        .code-example {\n",
    "            background: #2d2d2d;\n",
    "            color: #f8f8f2;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            font-family: 'Courier New', monospace;\n",
    "            font-size: 13px;\n",
    "            overflow-x: auto;\n",
    "            margin: 15px 0;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <div class=\"fujifilm-logo\">FUJIFILM</div>\n",
    "        <h1>üõë Stop Sequences Demonstration - OpenAI GPT-4</h1>\n",
    "        <p style=\"margin: 10px 0 0 0; font-size: 18px;\">Controlling AI Output Length and Content</p>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"info-banner\">\n",
    "        <strong>‚ö†Ô∏è Note:</strong> OpenAI's chat completions API supports the <code>stop</code> parameter to halt generation \n",
    "        at specific sequences. This demonstration shows how different stop sequences affect output.\n",
    "    </div>\n",
    "    \n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th style=\"width: 18%;\">Scenario</th>\n",
    "                <th style=\"width: 15%;\">Stop Sequences</th>\n",
    "                <th style=\"width: 47%;\">Generated Response</th>\n",
    "                <th style=\"width: 10%; text-align: center;\">Chars</th>\n",
    "                <th style=\"width: 10%; text-align: center;\">Status</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Test each scenario\n",
    "    for i, scenario in enumerate(scenarios, 1):\n",
    "        print(f\"\\nüõë SCENARIO {i}: {scenario['name']}\")\n",
    "        print(f\"üéØ Prompt: {scenario['prompt']}\")\n",
    "        print(f\"üõë Stop Sequences: {scenario['stop_sequences']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # OpenAI API call with stop parameter\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": scenario['prompt']}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=400,\n",
    "                stop=scenario['stop_sequences']  # Native stop parameter support\n",
    "            )\n",
    "            \n",
    "            output = response.choices[0].message.content\n",
    "            finish_reason = response.choices[0].finish_reason\n",
    "            char_length = len(output)\n",
    "            \n",
    "            # Determine if stopped by sequence or completed naturally\n",
    "            if finish_reason == \"stop\" and scenario['stop_sequences']:\n",
    "                status = \"STOPPED\"\n",
    "                status_indicator = \"truncated-indicator\"\n",
    "                stopped_at = \"at stop sequence\"\n",
    "            elif finish_reason == \"length\":\n",
    "                status = \"MAX TOKENS\"\n",
    "                status_indicator = \"truncated-indicator\"\n",
    "                stopped_at = \"max tokens reached\"\n",
    "            else:\n",
    "                status = \"COMPLETE\"\n",
    "                status_indicator = \"complete-indicator\"\n",
    "                stopped_at = \"completed naturally\"\n",
    "            \n",
    "            # Print to console\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output)\n",
    "            print(f\"\\nüìä Response Length: {char_length} characters\")\n",
    "            print(f\"üìä Finish Reason: {finish_reason}\")\n",
    "            print(f\"üìä Status: {stopped_at}\")\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'scenario': scenario['name'],\n",
    "                'prompt': scenario['prompt'],\n",
    "                'stop_sequences': scenario['stop_sequences'],\n",
    "                'output': output,\n",
    "                'char_length': char_length,\n",
    "                'status': status\n",
    "            })\n",
    "            \n",
    "            # Format stop sequences for display\n",
    "            if scenario['stop_sequences']:\n",
    "                stop_display = ', '.join([f'\"{s}\"' for s in scenario['stop_sequences']])\n",
    "                stop_class = \"stop-sequences\"\n",
    "            else:\n",
    "                stop_display = \"None\"\n",
    "                stop_class = \"stop-sequences no-stop\"\n",
    "            \n",
    "            # Add row to HTML table\n",
    "            html_output += f\"\"\"\n",
    "            <tr>\n",
    "                <td>\n",
    "                    <div class=\"scenario-name\">{scenario['name']}</div>\n",
    "                    <div class=\"prompt-text\">{scenario['prompt']}</div>\n",
    "                </td>\n",
    "                <td>\n",
    "                    <div class=\"{stop_class}\">{stop_display}</div>\n",
    "                </td>\n",
    "                <td>\n",
    "                    <div class=\"output-text\">{output}</div>\n",
    "                </td>\n",
    "                <td class=\"stats\">{char_length}</td>\n",
    "                <td class=\"stats\">\n",
    "                    <span class=\"{status_indicator}\">{status}</span>\n",
    "                </td>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            html_output += f\"\"\"\n",
    "            <tr>\n",
    "                <td>\n",
    "                    <div class=\"scenario-name\">{scenario['name']}</div>\n",
    "                </td>\n",
    "                <td colspan=\"4\" style=\"color: red;\">Error: {str(e)}</td>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    # Close table\n",
    "    html_output += \"\"\"\n",
    "        </tbody>\n",
    "    </table>\n",
    "\"\"\"\n",
    "    \n",
    "    # Add comparison section\n",
    "    html_output += \"\"\"\n",
    "    <div class=\"comparison-section\">\n",
    "        <div class=\"comparison-card\">\n",
    "            <h3>üéØ Without Stop Sequences</h3>\n",
    "            <p><strong>Behavior:</strong> AI continues until it naturally completes the response or hits max_tokens limit.</p>\n",
    "            <p><strong>Use Cases:</strong></p>\n",
    "            <ul>\n",
    "                <li>Full product descriptions</li>\n",
    "                <li>Complete blog posts</li>\n",
    "                <li>Comprehensive documentation</li>\n",
    "            </ul>\n",
    "            <p><strong>Example:</strong> \"Write a complete guide about Fujifilm printers\"</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"comparison-card\">\n",
    "            <h3>üõë With Stop Sequences</h3>\n",
    "            <p><strong>Behavior:</strong> AI stops immediately when it generates any of the specified sequences.</p>\n",
    "            <p><strong>Use Cases:</strong></p>\n",
    "            <ul>\n",
    "                <li>Limiting to specific sections</li>\n",
    "                <li>Generating structured lists</li>\n",
    "                <li>Controlling output format</li>\n",
    "            </ul>\n",
    "            <p><strong>Example:</strong> \"List features\" with stop=[\"5.\"] to get only first 4 items</p>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "    \n",
    "    # Add code example\n",
    "    html_output += \"\"\"\n",
    "    <div class=\"info-box\">\n",
    "        <h3>üíª Code Example - Using Stop Sequences</h3>\n",
    "        \n",
    "        <div class=\"code-example\">response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7,\n",
    "    max_tokens=400,\n",
    "    stop=[\"5.\", \"Benefits:\", \"###\"]  # <-- Stop sequences\n",
    ")\n",
    "\n",
    "# AI will stop at first occurrence of any sequence</div>\n",
    "        \n",
    "        <h3 style=\"margin-top: 25px;\">üéØ Common Use Cases for Stop Sequences</h3>\n",
    "        \n",
    "        <h4>1. Limiting List Items</h4>\n",
    "        <p><strong>Prompt:</strong> \"List the top 10 features of Apeos printers\"</p>\n",
    "        <p><strong>Stop:</strong> <code>[\"5.\"]</code> - Gets only first 4 items</p>\n",
    "        \n",
    "        <h4>2. Stopping at Section Headers</h4>\n",
    "        <p><strong>Prompt:</strong> \"Write product documentation with sections: Overview, Features, Setup\"</p>\n",
    "        <p><strong>Stop:</strong> <code>[\"Features:\", \"Setup:\"]</code> - Gets only Overview section</p>\n",
    "        \n",
    "        <h4>3. Preventing Unwanted Content</h4>\n",
    "        <p><strong>Prompt:</strong> \"Describe our printers but don't discuss competitors\"</p>\n",
    "        <p><strong>Stop:</strong> <code>[\"competitor\", \"vs\", \"compared to\"]</code> - Prevents comparisons</p>\n",
    "        \n",
    "        <h4>4. Structured Format Control</h4>\n",
    "        <p><strong>Prompt:</strong> \"Generate product specs in table format\"</p>\n",
    "        <p><strong>Stop:</strong> <code>[\"---\", \"###\"]</code> - Stops at markdown separators</p>\n",
    "        \n",
    "        <h4>5. Multiple Stop Points</h4>\n",
    "        <p><strong>Prompt:</strong> \"Explain camera features: sensor, lens, battery, screen\"</p>\n",
    "        <p><strong>Stop:</strong> <code>[\"battery\", \"screen\"]</code> - Stops at first match (sensor and lens only)</p>\n",
    "        \n",
    "        <h3 style=\"margin-top: 25px;\">‚ö†Ô∏è Important Notes</h3>\n",
    "        <ul>\n",
    "            <li><strong>First Match Wins:</strong> AI stops at the earliest occurrence of ANY stop sequence</li>\n",
    "            <li><strong>Exact Match:</strong> Stop sequences are case-sensitive: \"Stop\" ‚â† \"stop\"</li>\n",
    "            <li><strong>Maximum of 4:</strong> OpenAI allows up to 4 stop sequences per request</li>\n",
    "            <li><strong>Partial Words:</strong> Be careful - stop=\"the\" will trigger on \"the\", \"them\", \"their\", etc.</li>\n",
    "            <li><strong>Whitespace Matters:</strong> \"5.\" is different from \"5. \" (with space)</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3 style=\"margin-top: 25px;\">üí° Best Practices for Fujifilm Content</h3>\n",
    "        <ul>\n",
    "            <li><strong>Product Lists:</strong> Use numbered stops like \"5.\" or \"10.\" to control list length</li>\n",
    "            <li><strong>Documentation:</strong> Use section markers like \"###\" or \"---\" as stop points</li>\n",
    "            <li><strong>Structured Content:</strong> Define clear section headers and use them as stops</li>\n",
    "            <li><strong>Cost Control:</strong> Stop sequences save tokens = reduce API costs</li>\n",
    "            <li><strong>Format Consistency:</strong> Combine with prompts that define output structure</li>\n",
    "        </ul>\n",
    "        \n",
    "        <p style=\"margin-top: 20px;\"><strong>Generated:</strong> \"\"\" + time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"\"\"</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    \n",
    "    # Save HTML to file\n",
    "    output_file = \"stop_sequences_demo_gpt.html\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_output)\n",
    "    \n",
    "    print(f\"\\n‚úÖ HTML table saved to: {output_file}\")\n",
    "    print(\"üìÑ Open the file in your browser to view the formatted results!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Alternative: Console-only output\n",
    "def stop_sequences_demo_console():\n",
    "    \"\"\"Console-only demonstration of stop sequences\"\"\"\n",
    "    print(\"üõë Stop Sequences Demonstration\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    scenarios = [\n",
    "        {\"name\": \"No Stop Sequences\", \"prompt\": \"List FUJIFILM Apeos C325 features:\", \"stop_sequences\": None},\n",
    "        {\"name\": \"Stop at numbered item\", \"prompt\": \"List FUJIFILM Apeos C325 features:\", \"stop_sequences\": [\"4.\"]},\n",
    "        {\"name\": \"Stop at specific word\", \"prompt\": \"Write about FUJIFILM Instax cameras, including Mini, Square, and Wide.\", \"stop_sequences\": [\"Square\"]},\n",
    "        {\"name\": \"Multiple stop sequences\", \"prompt\": \"Explain FUJIFILM products: cameras, printers, medical, optical.\", \"stop_sequences\": [\"medical\", \"optical\"]},\n",
    "    ]\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\nüõë SCENARIO: {scenario['name']}\")\n",
    "        print(f\"üéØ Prompt: {scenario['prompt']}\")\n",
    "        print(f\"üõë Stop Sequences: {scenario['stop_sequences']}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": scenario['prompt']}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=400,\n",
    "                stop=scenario['stop_sequences']\n",
    "            )\n",
    "\n",
    "            output = response.choices[0].message.content\n",
    "\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output)\n",
    "            print(f\"\\nüìä Response Length: {len(output)} characters\")\n",
    "            print(f\"üìä Finish Reason: {response.choices[0].finish_reason}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Run the demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Choose output format:\")\n",
    "    print(\"1. HTML Table (saved to file)\")\n",
    "    print(\"2. Console Output Only\")\n",
    "    \n",
    "    choice = input(\"\\nEnter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        results = stop_sequences_demo_html()\n",
    "    else:\n",
    "        stop_sequences_demo_console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10036e5",
   "metadata": {},
   "source": [
    "## Stop Sequences Demonstration\n",
    "Show how stop sequences can control when generation ends, useful for structured outputs and preventing unwanted content continuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def combined_parameters_demo():\n",
    "    \"\"\"Show how temperature + top-p + max tokens work together using ChatGPT\"\"\"\n",
    "    print(\"üéõÔ∏è  Combined Parameters Optimization\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    base_prompt = (\n",
    "        \"Create a professional email responding to a job interview invitation. \"\n",
    "        \"Be polite and professional.\"\n",
    "    )\n",
    "\n",
    "    configurations = [\n",
    "        {\n",
    "            \"name\": \"Conservative (Formal/Predictable)\",\n",
    "            \"config\": {\n",
    "                \"temperature\": 0.2,\n",
    "                \"top_p\": 0.4,\n",
    "                \"max_output_tokens\": 200\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Balanced (Professional/Flexible)\",\n",
    "            \"config\": {\n",
    "                \"temperature\": 0.5,\n",
    "                \"top_p\": 0.7,\n",
    "                \"max_output_tokens\": 250\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Creative (Expressive/Varied)\",\n",
    "            \"config\": {\n",
    "                \"temperature\": 0.8,\n",
    "                \"top_p\": 0.9,\n",
    "                \"max_output_tokens\": 300\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(f\"üéØ Base Prompt: {base_prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    for cfg in configurations:\n",
    "        print(f\"\\nüéõÔ∏è  CONFIGURATION: {cfg['name']}\")\n",
    "        print(f\"‚öôÔ∏è  Settings: {json.dumps(cfg['config'], indent=2)}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                input=base_prompt,\n",
    "                temperature=cfg[\"config\"][\"temperature\"],\n",
    "                top_p=cfg[\"config\"][\"top_p\"],\n",
    "                max_output_tokens=cfg[\"config\"][\"max_output_tokens\"]\n",
    "            )\n",
    "\n",
    "            output = response.output_text\n",
    "\n",
    "            print(\"üìù Generated Response:\")\n",
    "            print(output)\n",
    "\n",
    "            print(f\"\\nüìä Response Length: {len(output)} characters\")\n",
    "            print(f\"üìä Word Count: {len(output.split())} words\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "# Run it\n",
    "combined_parameters_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33475908",
   "metadata": {},
   "source": [
    "## Parameter Impact Analysis Tool\n",
    "Build a reusable tool to systematically test parameter effects and analyze their impact on response characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783505e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "\n",
    "class ParameterAnalyzer:\n",
    "    \"\"\"Tool to analyze how different parameter values affect ChatGPT responses\"\"\"\n",
    "\n",
    "    def __init__(self, model=\"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.results = []\n",
    "\n",
    "    def test_parameter(self, prompt: str, param_name: str, values: list):\n",
    "        \"\"\"Test a specific OpenAI-supported parameter with various values\"\"\"\n",
    "        print(f\"üî¨ Testing {param_name.upper()} Parameter\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        for value in values:\n",
    "            try:\n",
    "                # Build dynamic argument dictionary\n",
    "                args = {\n",
    "                    \"model\": self.model,\n",
    "                    \"input\": prompt,\n",
    "                    \"max_output_tokens\": 200\n",
    "                }\n",
    "\n",
    "                # Add the dynamic parameter under test\n",
    "                args[param_name] = value\n",
    "\n",
    "                # Call the API\n",
    "                response = client.responses.create(**args)\n",
    "                text = response.output_text\n",
    "\n",
    "                result = {\n",
    "                    \"parameter\": param_name,\n",
    "                    \"value\": value,\n",
    "                    \"response_length\": len(text),\n",
    "                    \"word_count\": len(text.split()),\n",
    "                    \"response_preview\": (\n",
    "                        text[:100] + \"...\" if len(text) > 100 else text\n",
    "                    )\n",
    "                }\n",
    "\n",
    "                self.results.append(result)\n",
    "\n",
    "                print(\n",
    "                    f\"{param_name}={value}: \"\n",
    "                    f\"{result['word_count']} words, \"\n",
    "                    f\"'{result['response_preview']}'\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"{param_name}={value}: Error - {e}\")\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Return summary of last tests\"\"\"\n",
    "        if not self.results:\n",
    "            return \"No results to analyze.\"\n",
    "\n",
    "        summary = \"\\nüìä Parameter Analysis Summary:\\n\"\n",
    "        for r in self.results[-3:]:  # last 3 results\n",
    "            summary += (\n",
    "                f\"   {r['parameter']}={r['value']}: \"\n",
    "                f\"{r['word_count']} words\\n\"\n",
    "            )\n",
    "        return summary\n",
    "\n",
    "\n",
    "# ---------------------- Test the Analyzer ----------------------\n",
    "\n",
    "analyzer = ParameterAnalyzer()\n",
    "\n",
    "test_prompt = \"Explain machine learning in one paragraph.\"\n",
    "\n",
    "# Temperature is supported by OpenAI\n",
    "analyzer.test_parameter(test_prompt, \"temperature\", [0.1, 0.5, 0.9])\n",
    "\n",
    "print(analyzer.get_summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474da65",
   "metadata": {},
   "source": [
    "## Parameter Tuning Best Practices & Recommendations\n",
    "Summarize the entire demonstration and provide key takeaways and best practices for using Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Parameter Tuning Best Practices & Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_practices = {\n",
    "    \"Text Completion\": {\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.8,\n",
    "        \"purpose\": \"Balanced creativity with coherence\"\n",
    "    },\n",
    "    \"Code Generation\": {\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.5,\n",
    "        \"purpose\": \"High precision with minimal randomness\"\n",
    "    },\n",
    "    \"Creative Writing\": {\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.9,\n",
    "        \"purpose\": \"High creativity, expressive, varied output\"\n",
    "    },\n",
    "    \"Technical Documentation\": {\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.6,\n",
    "        \"purpose\": \"Reliable, accurate, consistent tone\"\n",
    "    },\n",
    "    \"Conversational AI\": {\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.8,\n",
    "        \"purpose\": \"Natural, human-like, engaging responses\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for use_case, params in best_practices.items():\n",
    "    print(f\"\\nüìã {use_case}:\")\n",
    "    print(f\"   ‚Ä¢ Temperature : {params['temperature']}\")\n",
    "    print(f\"   ‚Ä¢ Top-p       : {params['top_p']}\")\n",
    "    print(f\"   ‚Ä¢ Purpose     : {params['purpose']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
